{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from conlleval import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TransformerBlock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        maxlen = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NER Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super(NERModel, self).__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (C:\\Users\\dongw\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n",
      "100%|██████████| 3/3 [00:00<00:00, 111.06it/s]\n"
     ]
    }
   ],
   "source": [
    "conll_data = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_file(export_file_path, data):\n",
    "    with open(export_file_path, \"w\") as f:\n",
    "        for record in data:\n",
    "            ner_tags = record[\"ner_tags\"]\n",
    "            # for i in range(len(ner_tags)):\n",
    "            #     if ner_tags[i] in {8, 9}:\n",
    "            #         ner_tags[i] = 1\n",
    "            tokens = record[\"tokens\"]\n",
    "            if len(tokens) > 0:\n",
    "                f.write(\n",
    "                    str(len(tokens))\n",
    "                    + \"\\t\"\n",
    "                    + \"\\t\".join(tokens)\n",
    "                    + \"\\t\"\n",
    "                    + \"\\t\".join(map(str, ner_tags))\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "\n",
    "\n",
    "if not os.path.isdir('data'):\n",
    "    os.mkdir(\"data\")\n",
    "export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n",
    "export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'B-MISC', 9: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "def make_tag_lookup_table():\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
    "    res_dict = dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
    "    return res_dict\n",
    "\n",
    "\n",
    "mapping = make_tag_lookup_table()\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21009\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
    "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
    "\n",
    "counter = Counter(all_tokens_array)\n",
    "print(len(counter))\n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 20000\n",
    "\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
    "\n",
    "# Convert tokens to token IDs\n",
    "lookup_layer = keras.layers.StringLookup(\n",
    "    vocabulary=vocabulary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.TextLineDataset(\"./data/conll_train.txt\")\n",
    "val_data = tf.data.TextLineDataset(\"./data/conll_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_record_to_training_data(record):\n",
    "    record = tf.strings.split(record, sep=\"\\t\")\n",
    "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
    "    tokens = record[1 : length + 1]\n",
    "    tags = record[length + 1 :]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
    "    tags += 1\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "def lowercase_and_convert_to_ids(tokens):\n",
    "    tokens = tf.strings.lower(tokens)\n",
    "    return lookup_layer(tokens)\n",
    "\n",
    "\n",
    "# We use `padded_batch` here because each record in the dataset has a\n",
    "# different length.\n",
    "batch_size = 32\n",
    "train_dataset = (\n",
    "    train_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")\n",
    "val_dataset = (\n",
    "    val_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")\n",
    "\n",
    "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "loss = CustomNonPaddingTokenLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dongw\\miniconda3\\envs\\ml\\lib\\site-packages\\keras\\backend.py:4906: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439/439 [==============================] - 10s 16ms/step - loss: 0.5962\n",
      "Epoch 2/10\n",
      "439/439 [==============================] - 7s 15ms/step - loss: 0.2484\n",
      "Epoch 3/10\n",
      "439/439 [==============================] - 7s 15ms/step - loss: 0.1496\n",
      "Epoch 4/10\n",
      "439/439 [==============================] - 6s 14ms/step - loss: 0.1148\n",
      "Epoch 5/10\n",
      "439/439 [==============================] - 6s 14ms/step - loss: 0.0923\n",
      "Epoch 6/10\n",
      "439/439 [==============================] - 6s 14ms/step - loss: 0.0761\n",
      "Epoch 7/10\n",
      "439/439 [==============================] - 7s 16ms/step - loss: 0.0649\n",
      "Epoch 8/10\n",
      "439/439 [==============================] - 7s 15ms/step - loss: 0.0541\n",
      "Epoch 9/10\n",
      "439/439 [==============================] - 7s 15ms/step - loss: 0.0477\n",
      "Epoch 10/10\n",
      "439/439 [==============================] - 8s 17ms/step - loss: 0.0429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25486079dc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
    "ner_model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51362 tokens with 5942 phrases; found: 5748 phrases; correct: 4024.\n",
      "accuracy:  65.49%; (non-O)\n",
      "accuracy:  93.40%; precision:  70.01%; recall:  67.72%; FB1:  68.85\n",
      "              LOC: precision:  81.53%; recall:  82.20%; FB1:  81.87  1852\n",
      "             MISC: precision:  71.74%; recall:  64.97%; FB1:  68.18  835\n",
      "              ORG: precision:  58.25%; recall:  63.98%; FB1:  60.98  1473\n",
      "              PER: precision:  66.56%; recall:  57.38%; FB1:  61.63  1588\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(dataset):\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "\n",
    "    for x, y in dataset:\n",
    "        output = ner_model.predict(x)\n",
    "        predictions = np.argmax(output, axis=-1)\n",
    "        predictions = np.reshape(predictions, [-1])\n",
    "\n",
    "        true_tag_ids = np.reshape(y, [-1])\n",
    "\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "\n",
    "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
    "\n",
    "    evaluate(real_tags, predicted_tags)\n",
    "\n",
    "\n",
    "calculate_metrics(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[  988 10950   204   628     6  3938   215  5773]], shape=(1, 8), dtype=int64)\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_convert_to_ids(text):\n",
    "    tokens = text.split()\n",
    "    return lowercase_and_convert_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 65). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ner.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ner.model\\assets\n"
     ]
    }
   ],
   "source": [
    "ner_model.save('ner.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['eu'], ['organization'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_iob_to_entities(tokens, iobs):\n",
    "    length = len(tokens)\n",
    "    entities = []\n",
    "    entity_types = []\n",
    "    entity_type = None\n",
    "    entity = []\n",
    "    entity_dict = {'LOC': 'address', 'ORG': 'organization', 'PER': 'person'}\n",
    "    for i in range(length):\n",
    "        temp = iobs[i].split('-')\n",
    "        if len(temp) == 1:\n",
    "            continue\n",
    "        symbol = temp[0]\n",
    "        e_type = temp[1]\n",
    "        if e_type in entity_dict:\n",
    "            entity.append(tokens[i])\n",
    "            entity_type = entity_dict[e_type]\n",
    "            if i == length-1 or iobs[i+1] == 'O' or iobs[i+1][0] == 'B':\n",
    "                entities.append(' '.join(entity))\n",
    "                entity_types.append(entity_type)\n",
    "                entity = []\n",
    "                entity_type = None\n",
    "    return entities, entity_types\n",
    "\n",
    "\n",
    "def get_entities(sent, model):\n",
    "    input = tokenize_and_convert_to_ids(sent)\n",
    "    input = tf.reshape(input, shape=[1, -1])\n",
    "    output = model.predict(input)\n",
    "    prediction = np.argmax(output, axis=-1)[0]\n",
    "    prediction = [mapping[i] for i in prediction]\n",
    "    entities, entity_types = convert_iob_to_entities(sent.split(), prediction)\n",
    "    return entities, entity_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = \"eu rejects german call to boycott british lamb.\"\n",
    "get_entities(sample_input, ner_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recognize entities in database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from database import DATABASE_NAME\n",
    "con = sqlite3.connect(DATABASE_NAME)\n",
    "cur = con.cursor()\n",
    "cur.execute('''\n",
    "SELECT rowid, sent from sentence;\n",
    "''')\n",
    "rows = cur.fetchall()\n",
    "for id, sent in rows:\n",
    "    records = []\n",
    "    entities, entity_types = get_entities(sent, ner_model)\n",
    "    for entity, entity_type in zip(entities, entity_types):\n",
    "        records.append((entity, entity_type, id))\n",
    "    cur.executemany('''\n",
    "    INSERT INTO ner (entity, entity_type, sentence_id)\n",
    "    VALUES(?, ?, ?);''', records)\n",
    "con.commit()\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1909392cfcb85456012e1424d7555787e32b0810029f7da2d6fb627b1043987b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
